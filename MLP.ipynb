{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "In this assignment, you are required to implement a **fully connected neural network** (using NumPy) to perform image classification on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "# Load the MNIST data and flatten it\n",
    "mnist_dir = \"../data/\"\n",
    "\n",
    "from data.mnist import load_from_pickle, load_and_pickle\n",
    "load_and_pickle(mnist_dir)\n",
    "data = load_from_pickle(os.path.join(mnist_dir + 'mnist.pkl'))\n",
    "\n",
    "trainX, trainy = data['trainX'], data['trainy']\n",
    "testX, testy = data['testX'], data['testy']\n",
    "\n",
    "trainX = trainX.reshape(len(trainX), -1).astype('int')\n",
    "testX = testX.reshape(len(testX), -1).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Network\n",
    "Next, you are first going to design, debug & implement a neural network with fully-connected layers to perform classification using the following \"toy data\". Following this,  you will test the neural network on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a small neural network and use the “toy data” below to verify its functionality.\n",
    "# Note that the random seed is set to ensure that the experiments are repeatable\n",
    "\n",
    "from network import Net\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return Net(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass: Compute scores\n",
    "Open the file `network.py` and carefully look at the class method `Net.loss` which uses the data and weights to compute the class scores, the loss, and the gradients. Compute the scores for all inputs in the forward pass using the implementation and then run the cell below to verify your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      " correct scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      " Difference between your scores and correct scores:\n",
      "3.6802720745909845e-08\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "\n",
    "print('\\n correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "\n",
    "# The difference should be very small;  < 1e-7\n",
    "print('\\n Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass: Compute loss\n",
    "Complete the second part in the same class method `Net.loss` which computes the data and regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your loss and correct loss:\n",
      "1.7985612998927536e-13\n"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.05)\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "# should be very small; less than 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "Complete the rest of `Net.loss` (i.e., the backward pass) to compute the gradient of the loss with respect to variables `W1`, `b1`, `W2`, and `b2`. Next, debug your backward pass using the following numeric gradient check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 max relative error: 3.440708e-09\n",
      "b2 max relative error: 4.447656e-11\n",
      "W1 max relative error: 3.561318e-09\n",
      "b1 max relative error: 2.738421e-09\n"
     ]
    }
   ],
   "source": [
    "from gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.05)\n",
    "\n",
    "# these differences should be less than 1e-8\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network using Stochastic Gradient Descent (SGD)\n",
    "Complete the missing sections in class methods `Net.train` and `Net.predict` to implement the training and prediction procedures, respectively.\n",
    "\n",
    "Next, run the code below to train a two-layer network on \"toy data\". Take note of the training loss, which should be less than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss:  0.01563498761185671\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have now implemented a two-layer network tested using the \"toy data\", you should next train & test it on a real dataset such as the MNIST data. Run the code below to train your network using SGD and an adaptive learning rate schedule as the optimization proceeds, then test it on a validation set. After each epoch, reduce the learning rate by multiplying it by a decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 10000: loss 2.302665\n",
      "iteration 1000 / 10000: loss 0.756872\n",
      "iteration 2000 / 10000: loss 0.394492\n",
      "iteration 3000 / 10000: loss 0.319647\n",
      "iteration 4000 / 10000: loss 0.332465\n",
      "iteration 5000 / 10000: loss 0.322155\n",
      "iteration 6000 / 10000: loss 0.282419\n",
      "iteration 7000 / 10000: loss 0.354267\n",
      "iteration 8000 / 10000: loss 0.422917\n",
      "iteration 9000 / 10000: loss 0.310127\n",
      "Validation accuracy:  0.944\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val = trainX[:50000], trainX[50000:]\n",
    "y_train, y_val = trainy[:50000], trainy[50000:]\n",
    "\n",
    "input_size = 28 * 28\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = Net(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=10000, batch_size=128,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.25, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune your network\n",
    "Complete the code block below to tune your network on the validation set. You may use the cross validation method discussed in Assignment1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy:  0.9747166666666667\n",
      "{'hidden_size': 90, 'num_iters': 10000, 'batch_size': 256, 'learning_rate': 0.0025, 'learning_rate_decay': 0.95, 'reg': 0.1}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import copy\n",
    "from sklearn.model_selection import KFold\n",
    "best_net = None # store the best model into this \n",
    "\n",
    "#################################################################################\n",
    "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
    "# model in best_net.                                                            #\n",
    "#################################################################################\n",
    "# Enter your code\n",
    "k = 5    # for K-fold cross validation\n",
    "kf = KFold(n_splits=k)\n",
    "kf.get_n_splits(trainX)\n",
    "\n",
    "input_size = 28 * 28\n",
    "num_classes = 10\n",
    "\n",
    "best_val_acc = 0\n",
    "best_param = None\n",
    "# grid_param = {  \n",
    "#     'hidden_size': [30, 50, 70],\n",
    "#     'num_iters': [10000],\n",
    "#     'batch_size': [64, 128, 256, 1024],\n",
    "#     'learning_rate': [1e-3, 1e-4, 1e-5],\n",
    "#     'learning_rate_decay': [0.9, 0.95, 0.99],\n",
    "#     'reg': [0.1, 0.25, 0.5],   \n",
    "# }\n",
    "\n",
    "grid_param = {  \n",
    "    'hidden_size': [90],\n",
    "    'num_iters': [10000],\n",
    "    'batch_size': [256],\n",
    "    'learning_rate': [2.5e-3],\n",
    "    'learning_rate_decay': [0.95],\n",
    "    'reg': [0.1],   \n",
    "}\n",
    "\n",
    "keys = grid_param.keys()\n",
    "values = (grid_param[key] for key in keys)\n",
    "combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]\n",
    "\n",
    "for param in combinations:\n",
    "    val_acc = []\n",
    "    for train_index, test_index in kf.split(trainX):\n",
    "        X_train, X_val = trainX[train_index], trainX[test_index]\n",
    "        y_train, y_val = trainy[train_index], trainy[test_index]\n",
    "        net = Net(input_size, param['hidden_size'], num_classes)\n",
    "        # Train the network\n",
    "        stats = net.train(X_train, y_train, X_val, y_val, num_iters=param['num_iters'],\n",
    "                          batch_size=param['batch_size'], learning_rate=param['learning_rate'],\n",
    "                          learning_rate_decay=param['learning_rate_decay'], reg=param['reg'])\n",
    "        # Predict on the validation set\n",
    "        val_acc.append((net.predict(X_val) == y_val).mean())\n",
    "    avg_val_acc = sum(val_acc) / k\n",
    "    if avg_val_acc > best_val_acc:\n",
    "        best_val_acc = avg_val_acc\n",
    "        best_net = copy.deepcopy(net)\n",
    "        best_param = param\n",
    "\n",
    "print(\"Best Accuracy: \", best_val_acc)\n",
    "print(best_param)\n",
    "#################################################################################\n",
    "#                               END OF YOUR CODE                                #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on the test set\n",
    "Finally, evaluate your final trained network on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9776\n"
     ]
    }
   ],
   "source": [
    "test_acc = (best_net.predict(testX) == testy).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
